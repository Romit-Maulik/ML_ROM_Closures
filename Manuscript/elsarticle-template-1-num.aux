\relax 
\emailauthor{rmaulik@anl.gov}{Romit Maulik\corref {cor1}}
\emailauthor{arvindm@lanl.gov}{Arvind Mohan}
\emailauthor{blusch@anl.gov}{Bethany Lusch}
\emailauthor{smadireddy@anl.gov}{Sandeep Madireddy}
\emailauthor{pbalapra@anl.gov}{Prasanna Balaprakash}
\emailauthor{livescu@lanl.gov}{Daniel Livescu}
\Newlabel{cor1}{1}
\Newlabel{label1}{a}
\Newlabel{label2}{b}
\Newlabel{label3}{c}
\citation{carlberg2011efficient,wang2012proper,san2015principal,ballarin2015supremizer,san2018extreme,wang2019non,choi2019space}
\citation{proctor2016dynamic}
\citation{peherstorfer2016optimal}
\citation{sapsis2013statistically,zahr2018efficient}
\citation{wells2017evolve,xie2018data,san2018neural,san2019artificial}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\newlabel{Intro}{{1}{2}}
\newlabel{gen1}{{1}{2}}
\citation{kosambi1943statistics,berkooz1993proper}
\citation{taira2019modal}
\newlabel{gen3}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Proper orthogonal decomposition}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Galerkin-projection onto reduced space}{5}}
\citation{lee2019model}
\citation{lusch2018deep,mohan2018deep,san2018neural,san2018extreme,guo2019data,san2019artificial,mohebujjaman2019physically,wang2019non,yeo2019deep,mohan2019compressed}
\citation{brunton2016discovering,kutz2016dynamic,rudy2017data,champion2019data,raissi2017physics}
\citation{ma2018model}
\citation{wang2019recurrent}
\citation{san2018neural}
\citation{bec2007burgers,maulik2018explicit}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contribution}{6}}
\citation{haber2017stable,ruthotto2018deep,behrmann2018invertible,reshniak2019robust}
\citation{chen2018neural}
\citation{chen2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {2}Latent-space learning}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural ordinary differential equations}{7}}
\newlabel{adj}{{21}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Long short-term memory networks}{9}}
\citation{mori1965transport,zwanzig1973nonlinear}
\citation{evans2008statistical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Connection with Mori-Zwanzig formalism}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{11}}
\citation{san2018neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Advecting shock}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Full-order solution of the advecting shock problem. Note the presence of a moving discontinuity.}}{13}}
\newlabel{Figure1}{{1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and validation loss convergence with epochs for time-series predictions of the advecting shock case.}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {LSTM}}}{14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {NODE}}}{14}}
\newlabel{Figure2}{{2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces POD-space coefficient evolution for the advecting shock case.}}{15}}
\newlabel{Figure3}{{3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Field reconstruction ability for NODE and LSTM for the advecting shock case.}}{16}}
\newlabel{Figure4}{{4}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Burgers' turbulence}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A comparison of three different LSTM predictions for the advecting shock case.}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Field}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed}}}{17}}
\newlabel{Figure5}{{5}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A comparison of three different NODE predictions for the advecting shock case. The deployment with 16 neurons coincides with the true solution.}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Zoomed out}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed in}}}{17}}
\newlabel{Figure6}{{6}{17}}
\citation{san2013stationary}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ensemble training and validation losses for the LSTM architecture for the advecting shock case.}}{18}}
\newlabel{Figure7}{{7}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ensemble training and validation losses for the NODE architecture for the advecting shock case.}}{19}}
\newlabel{Figure8}{{8}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Initial and final conditions for the \emph  {Burgulence} case showing multiple standing discontinuities decaying in strength over time.}}{20}}
\newlabel{Figure9}{{9}{20}}
\citation{balaprakash2018deephyper}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Improving performance through hyperparameter search}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces POD-space coefficient evolution for the \emph  {Burgulence} case.}}{22}}
\newlabel{Figure10}{{10}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Kinetic-energy spectra predictions (left) and their residuals (right) as predicted by NODE and LSTM.}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Spectra}}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Residual}}}{23}}
\newlabel{Figure11}{{11}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Field reconstruction abilities for the NODE and LSTM frameworks showing superior performance as compared to GP.}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Zoomed out}}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed in}}}{23}}
\newlabel{Figure12}{{12}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Search range for LSTM hyperparameters and their optimal values deployed for the Burgers' turbulence test case.}}{24}}
\newlabel{Table1}{{1}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Search range for NODE hyperparameters and their optimal values deployed for the Burgers' turbulence test case.}}{24}}
\newlabel{Table2}{{2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Pairwise dependency plots for LSTM hyperparameter search using Deephyper. Diagonal entries show distributions of configurations sampled. Note that loss is encoded as negative since the hyperparameter search is based on objective function maximization.}}{25}}
\newlabel{Figure13}{{13}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Pairwise LSTM hyperparameter correlations for the Burgers turbulence case.}}{26}}
\newlabel{Figure14}{{14}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Pairwise dependency plots for NODE hyperparameter search using Deephyper. Diagonal entries show distributions of configurations sampled. Note that loss is encoded as negative since the hyperparameter search is based on objective function maximization.}}{27}}
\newlabel{Figure15}{{15}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Pairwise NODE hyperparameter correlations for the Burgers turbulence case.}}{28}}
\newlabel{Figure16}{{16}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces POD-space coefficient evolution for the \emph  {Burgulence} case with improved hyperparameter choices. The LSTM performance is significantly improved.}}{29}}
\newlabel{Figure17}{{17}{29}}
\citation{lee2019model}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Kinetic-energy spectra predictions (left) and their residuals (right) as predicted by NODE and LSTM deployed with optimal hyperparameters.}}{30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Spectra}}}{30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Residual}}}{30}}
\newlabel{Figure18}{{18}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion and conclusions}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Data availability}{31}}
\bibstyle{model1-num-names}
\bibdata{references.bib}
\bibcite{carlberg2011efficient}{{1}{2011}{{Carlberg et~al.}}{{Carlberg, Bou-Mosleh, and Farhat}}}
\bibcite{wang2012proper}{{2}{2012}{{Wang et~al.}}{{Wang, Akhtar, Borggaard, and Iliescu}}}
\bibcite{san2015principal}{{3}{2015}{{San and Borggaard}}{{}}}
\bibcite{ballarin2015supremizer}{{4}{2015}{{Ballarin et~al.}}{{Ballarin, Manzoni, Quarteroni, and Rozza}}}
\bibcite{san2018extreme}{{5}{2018}{{San and Maulik}}{{}}}
\bibcite{wang2019non}{{6}{2019}{{Wang et~al.}}{{Wang, Hesthaven, and Ray}}}
\bibcite{choi2019space}{{7}{2019}{{Choi and Carlberg}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Acknowledgements}{32}}
\bibcite{proctor2016dynamic}{{8}{2016}{{Proctor et~al.}}{{Proctor, Brunton, and Kutz}}}
\bibcite{peherstorfer2016optimal}{{9}{2016}{{Peherstorfer et~al.}}{{Peherstorfer, Willcox, and Gunzburger}}}
\bibcite{sapsis2013statistically}{{10}{2013}{{Sapsis and Majda}}{{}}}
\bibcite{zahr2018efficient}{{11}{2018}{{Zahr et~al.}}{{Zahr, Carlberg, and Kouri}}}
\bibcite{wells2017evolve}{{12}{2017}{{Wells et~al.}}{{Wells, Wang, Xie, and Iliescu}}}
\bibcite{xie2018data}{{13}{2018}{{Xie et~al.}}{{Xie, Mohebujjaman, Rebholz, and Iliescu}}}
\bibcite{san2018neural}{{14}{2018}{{San and Maulik}}{{}}}
\bibcite{san2019artificial}{{15}{2019}{{San et~al.}}{{San, Maulik, and Ahmed}}}
\bibcite{kosambi1943statistics}{{16}{1943}{{Kosambi}}{{}}}
\bibcite{berkooz1993proper}{{17}{1993}{{Berkooz et~al.}}{{Berkooz, Holmes, and Lumley}}}
\bibcite{taira2019modal}{{18}{2019}{{Taira et~al.}}{{Taira, Hemati, Brunton, Sun, Duraisamy, Bagheri, Dawson, and Yeh}}}
\bibcite{lee2019model}{{19}{2019}{{Lee and Carlberg}}{{}}}
\bibcite{lusch2018deep}{{20}{2018}{{Lusch et~al.}}{{Lusch, Kutz, and Brunton}}}
\bibcite{mohan2018deep}{{21}{2018}{{Mohan and Gaitonde}}{{}}}
\bibcite{guo2019data}{{22}{2019}{{Guo and Hesthaven}}{{}}}
\bibcite{mohebujjaman2019physically}{{23}{2019}{{Mohebujjaman et~al.}}{{Mohebujjaman, Rebholz, and Iliescu}}}
\bibcite{yeo2019deep}{{24}{2019}{{Yeo and Melnyk}}{{}}}
\bibcite{mohan2019compressed}{{25}{2019}{{Mohan et~al.}}{{Mohan, Daniel, Chertkov, and Livescu}}}
\bibcite{brunton2016discovering}{{26}{2016}{{Brunton et~al.}}{{Brunton, Proctor, and Kutz}}}
\bibcite{kutz2016dynamic}{{27}{2016}{{Kutz et~al.}}{{Kutz, Brunton, Brunton, and Proctor}}}
\bibcite{rudy2017data}{{28}{2017}{{Rudy et~al.}}{{Rudy, Brunton, Proctor, and Kutz}}}
\bibcite{champion2019data}{{29}{2019}{{Champion et~al.}}{{Champion, Lusch, Kutz, and Brunton}}}
\bibcite{raissi2017physics}{{30}{2017}{{Raissi et~al.}}{{Raissi, Perdikaris, and Karniadakis}}}
\bibcite{ma2018model}{{31}{2018}{{Ma et~al.}}{{Ma, Wang et~al.}}}
\bibcite{wang2019recurrent}{{32}{2019}{{Wang et~al.}}{{Wang, Ripamonti, and Hesthaven}}}
\bibcite{bec2007burgers}{{33}{2007}{{Bec and Khanin}}{{}}}
\bibcite{maulik2018explicit}{{34}{2018}{{Maulik and San}}{{}}}
\bibcite{haber2017stable}{{35}{2017}{{Haber and Ruthotto}}{{}}}
\bibcite{ruthotto2018deep}{{36}{2018}{{Ruthotto and Haber}}{{}}}
\bibcite{behrmann2018invertible}{{37}{2018}{{Behrmann et~al.}}{{Behrmann, Duvenaud, and Jacobsen}}}
\bibcite{reshniak2019robust}{{38}{2019}{{Reshniak and Webster}}{{}}}
\bibcite{chen2018neural}{{39}{2018}{{Chen et~al.}}{{Chen, Rubanova, Bettencourt, and Duvenaud}}}
\bibcite{mori1965transport}{{40}{1965}{{Mori}}{{}}}
\bibcite{zwanzig1973nonlinear}{{41}{1973}{{Zwanzig}}{{}}}
\bibcite{evans2008statistical}{{42}{2008}{{Evans and Morriss}}{{}}}
\bibcite{san2013stationary}{{43}{2013}{{San and Staples}}{{}}}
\bibcite{balaprakash2018deephyper}{{44}{2018}{{Balaprakash et~al.}}{{Balaprakash, Salim, Uram, Vishwanath, and Wild}}}
